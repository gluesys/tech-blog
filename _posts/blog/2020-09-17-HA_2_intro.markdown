---
layout:     post
title:      "스토리지 이중화 2편: 스토리지 이중화 아키텍쳐 설계"
date:       2021-01-14
author:     김 태훈 (thkim@gluesys.com)
categories: blog
tags:       HA-Pacemaker-이중화-NAS-스토리지-HDD-SSD.
cover:      "/assets/HA1_maincover.jpg"
main:       "/assets/HA1_maincover.jpg"
---

안녕하세요. 지난번 포스팅 [고가용성과 이중화](https://tech.gluesys.com/blog/2020/08/22/HA_1_intro.html)에 이어서 이번 포스트에서는 클라이언트 환경에 따른 스토리지 이중화 아키텍처 설계 방법과 실 구축 사례를 소개하겠습니다.

&nbsp;

## NAS HA Architecture

NAS(Network Attached Storage)는 네트워크 기반의 파일 공유 서비스를 수행하는 스토리지이며, 클라이언트는 일반적으로 이더넷 네트워크를 통해 NAS에 접속하여 파일을 저장하거나 불러올 수 있습니다.

이를 위해 NAS는 스토리지 본연의 기능인 볼륨 구성 및 데이터 저장, 백업 등의 기술만이 아닌, 계정 관리와 인증, 파일 공유, 네트워크, 보안 등, 클라이언트 환경에 대한 인프라 기능을 제공해야합니다.

더 나아가 단일 스토리지가 아닌 **NAS 이중화 환경**으로 구성할 경우, 이중화를 위한 부수적인 기능들이 추가되어야 합니다. 이때 가장 중요한 요소는 **구축하고자 하는 목적과 클라이언트 인프라에 대한 이해**입니다. 지난 포스트에서 설명했듯이 잘못된 지점을 모니터링하거나 중요한 지점을 관리하지 못한다면 예상치 못한 장애로 failover가 발생할 수 있습니다.

따라서 NAS 이중화는 스토리지에 대한 기능과 파일 공유를 위한 다양한 요소들을 빠짐없이 모니터링해야하기 때문에 다른 이중화 서비스에 비교해서 보다 높은 기술 지식과 환경에 대한 이해를 요구합니다.

&nbsp;

## NAS 이중화 사례 설명

그렇다면 실제 구축된 사례를 예시로 NAS 이중화 아키텍쳐를 설계하는 과정을 설명해보겠습니다.

&nbsp;

![Alt text](/assets/HA2_FIG1.jpg){: width="500"}
<center>&#60;VDI 환경을 위한 NAS 이중화 시스템 구성도&#62;</center>

&nbsp;

위 그림은 ㅎ사에 구축된 업무 환경을 위한 VDI(Virtual Desktop Infrastructure) 기반의 시스템 구성도이며, 고객사에서 요구하는 기능은 다음과 같습니다.

> * VDI 클러스터는 가상머신의 이미지를 저장할 용도로 NFS 공유 환경을 제공받아야 함
> * 서비스 도중에 failover가 발생하더라도 실행중인 가상머신에 영향이 없어야 함
> * 가상머신에 설치된 운영체제는 윈도우즈 기반이며, CIFS(Common Interest File System) 공유 환경을 제공받아야 함
> * 계정 정보는 AD(Active Directory)로 통합 관리되며, NAS는 AD와 연동하여 사용자 인증을 처리해야 함
> * 동시 접속자는 약 1000명이며, 1000대의 가상머신과 파일 공유 환경을 동시에 제공해야 함

ㅎ사의 환경은 조금 특별했습니다. 고객사는 기존에 가상화 기술과 스토리지가 연동된 HCI(Hyper-Converged Infrastructure) 장비를 통해 가상머신의 이미지를 클러스터 스토리지에서 관리하고, 가상머신으로 NAS를 구축하여 사용자 파일을 공유하고 있었습니다. 하지만 잦은 장애와 부족한 NAS 성능으로 원활한 업무 환경 제공에 많은 노력이 필요했다고 합니다.

이번 VDI 구축 사업에는 인프라 구성에 필요한 가상머신 이미지와 개인 사용자 파일을 별도의 고성능 스토리지에 저장하고 공유하기를 원했습니다. 이 경우 NAS에 문제가 발생한다면 사용자 파일 공유뿐만 아니라, 인프라 전체가 다운될 수 있어서 NAS 이중화 환경 구성은 필수였습니다. 만약 장애가 발생하여 Standby 스토리지에 서비스가 이전(failover)되더라도 실행중인 가상머신에는 영향이 없어야 하기 때문에 런타임 서비스 이전(runtime failover)이 가능한 고성능 NAS 이중화 기술이 필요했습니다.

&nbsp;

![Alt text](/assets/HA2_FIG4.jpg){: width="500"}
<center>&#60;(좌)VDI 환경의 입출력 패턴 / (우)평균 IOPS 기준에 따른 성능 체감 기준&#62;</center>

&nbsp;

가상화 환경은 real-time small data 패턴의 입출력 요청(I/O Request)이 발생합니다. ㅎ사는 별도의 사용자 데이터가 저장될 공간을 가상머신 이미지로부터 분리했기 때문에 가상머신의 대부분의 입출력 요청은 사용자 파일을 위한 요청이 아닌 게스트 운영체제에서 요청한 것이며, 그 단위는 작게는 bytes에서 KB 정도의 크기입니다. 이번 구축 사례에서는 ㅎ사의 입출력 패턴에 최적화 하기위해 기존의 파일시스템 기반의 클러스터링 기술이 아닌 블록 기반의 데이터 미러링 기술을 사용했습니다. 클러스터 파일시스템의 공유 단위가 파일이라면, 블록 기반의 데이터 미러링은 실제 디스크에 저장되는 블록 단위로 저장되는 차이점이 있습니다.

&nbsp;

![Alt text](/assets/HA2_FIG2.jpg){: width="500"}
<center>&#60;블록 기반의 데이터 미러링 기술의 입출력 흐름도&#62;</center>

&nbsp;

위 그림은 블록 기반의 데이터 미러링 기술의 입출력 흐름도 입니다. 스토리지는 Active 노드에서 NAS 서비스를 수행하고 있으며, 클라이언트의 파일 공유 요청은 SMB와 NFS와 같은 공유 프로토콜과 다양한 레이어를 지나 소프트웨어 계층의 최하단인 데이터 미러링 스택에 도착합니다(1). 데이터 미러링에 전달된 블록 입출력 요청은 Active 노드의 디스크에서 처리하는 동시에 미러링된 Standby 노드에도 전달합니다(2). Standby 노드의 데이터 미러링 스택은 로컬 디스크에도 동일한 블록 입출력 요청을 처리하고 그 결과를 Active 노드에 전달합니다(4). Active 노드의 데이터 미러링 스택은 입출력 결과를 상위 계층에 전달하며(5) 입출력 요청을 완료합니다.

데이터 미러링 기술에서 가장 중요한 이슈는 성능과 안정성 입니다. 먼저 이중화된 노드 사이에 데이터 미러링에 소요되는 지연시간(Latency)을 최소화 하기위해 스토리지 미러링 네트워크는 인피니밴드(Infiniband)를 사용했습니다. 그리고 약1000명의 VM과 사용자로부터 발생하는 Random Access 패턴에 대한 소요되는 시간을 최소화하기위해 검색 시간(Seek Time)이 낮은 SSD로 디스크를 구성하여 저지연성을 확보했습니다. 이때 시스탬 내부의 장애 상황을 대비하여 인피니밴드는 본딩(Bonding) 기술을 통해 이중화로 구성했으며, SSD는 RAID 10로 구성하여 IOPS(Input/Ouput Operations Per Second)를 높이고 패리티 연산으로 소모되는 플래시 메모리 수명을 최소화하여 성능과 안정성을 확보했습니다.

그렇다면 위와 같은 고성능 NAS 이중화 구성 요소와 고객사에 필요한 인프라 구성 요소에 필요한 모니터링 스택을 설명하겠습니다. 모니터링 스택은 입출력 처리 흐름에 따른 순서와 각 요소별 관계에 따라서 설계해야 합니다. 아래 그림은 ㅎ사에 구축된 고성능 NAS 이중화 시스템의 아키텍처 입니다.

&nbsp;

![Alt text](/assets/HA2_FIG3.jpg){: width="500"}
<center>&#60;ㅎ사에 구축된 NAS 이중화 시스템의 모니터링 지점&#62;</center>

&nbsp;

위 그림의 최하단은 올 플래시로 구성된 RAID 볼륨 장치이며, 데이터 미러링 스택은 블록 장치에 가장 먼저 연결됩니다. 데이터 미러링 기술에는 리눅스에서 블록 기반의 데이터 이중화 기술로 인정받은 DRBD(Distributed Replicasted Storage System)을 사용합니다(DRBD에 대한 설명은 다른 포스트에서 자세히 다루도록 하겠습니다.) DRBD는 앞서 설명한 바와 같이 인피니밴드를 이용하여 지연시간을 최소화 하였으며, 확보된 성능을 활용하여 모든 입출력 요청이 완료된 이후에 어플리케이션에 완료 응답을 송신하는 안정적인 구조인 DRBD Protocol C 타입으로 미러링했습니다.

다음으로 ㅎ사에 도입된 NAS 이중화 시스템의 모니터링 지점과 소프트웨어 스택에 대한 순서와 관계성에 대해서 설명하겠습니다. 먼저 도입된 소프트웨어의 순서는 다음과 같습니다.

~~실제로 클러스터 파일시스템을 통해 데모 환경을 구성했을 때, failover 과정에서 가상머신에 블루스크린이 발생하여 시스템이 다운되었습니다.~~
하지만 블록 기반의 미러


위 환경은 조금 특별한 상황입니다. 먼저 

(사례를 통한 아키텍처 설명)
1. KT 사례 (모바일 스트리밍, 167TB, SAN, NFS, 40G, ethmonitor, FCmonitor, A/B)

2. 센트럴넷 사례 (대용량 미디어, 잔파일, SAN, Gluster, A/A)

3. 지역난방공사 사례 (VDI, All Flash Storage, AD, SMB, NFS, DRBD, A/B)

4. 심평원 사례 (빅데이터, 잔파일, SAN, NFS, personal_cmd,  A/B)

![Alt text](/assets/???.jpg){: width="500"}
<center>&#60;모바일 스트리밍 환경을 위한 A/B NAS 이중화 구축&#62;</center>

&nbsp;

## 이중화를 위한 오픈소스 프로젝트

(Pacemaker 소개)
(Pacemaker 기능 소개)
(Pacemaker를 통한 스토리지 이중화 환경 구축 실습-간단하게)

&nbsp;

## 마치며

(마무리 멘트)
이번 포스트에서는 사례별 NAS 이중화 아키텍처에 대해서 설명했으며, 이중화를 위한 오픈소스 프로젝트인 Pacemaker에 대한 소개와 핵심 기능들을 간단하게 소개했습니다. 그리고 앞서 설계한 아키텍처를 목표로 환경을 구축을 실습해보았습니다.

다음 포스트는 Pacemaker 심화 과정으로 실제 현장에서 발생한 이슈들과 이를 해결하기 위한 기술적인 접근 방법들을 공유하겠습니다. 더 나아가 특수 환경에서 중요한 지점을 모니터링하기 위해 Pacemaker 리소스를 개발하는 방법과 현재까지 내부에서 개발한 몇가지 리소스를 공개하도록 하겠습니다. 감사합니다.

&nbsp;

## 참고

 * https://www.netapp.com/media/19785-wp-sizing-storage-for-desktop.pdf
 * https://www.infortrend.com/ImageLoader/LoadDoc/509/True/True/Infortrend%20document

## 각주

[^1]: https://???
