---
layout:     post
title:      "스토리지 이중화 2편: 스토리지 이중화 아키텍쳐 설계"
date:       2021-01-14
author:     김 태훈 (thkim@gluesys.com)
categories: blog
tags:       HA-Pacemaker-이중화-NAS-스토리지-HDD-SSD.
cover:      "/assets/HA1_maincover.jpg"
main:       "/assets/HA1_maincover.jpg"
---

안녕하세요. 지난번 포스팅 [고가용성과 이중화](https://tech.gluesys.com/blog/2020/08/22/HA_1_intro.html)에 이어서 이번 포스트에서는 클라이언트 환경에 따른 스토리지 이중화 아키텍처 설계 방법과 실 구축 사례를 소개하겠습니다.

&nbsp;

## NAS HA Architecture

NAS(Network Attached Storage)는 네트워크 기반의 파일 공유 서비스를 수행하는 스토리지이며, 클라이언트는 일반적으로 이더넷 네트워크를 통해 NAS에 접속하여 파일을 저장하거나 불러올 수 있습니다.

이를 위해 NAS는 스토리지 본연의 기능인 볼륨 구성 및 데이터 저장, 백업 등의 기술만이 아닌, 계정 관리와 인증, 파일 공유, 네트워크, 보안 등, 클라이언트 환경에 대한 인프라 기능을 제공해야합니다.

더 나아가 단일 스토리지가 아닌 **NAS 이중화 환경**으로 구성할 경우, 이중화를 위한 부수적인 기능들이 추가되어야 합니다. 이때 가장 중요한 요소는 **구축하고자 하는 목적과 클라이언트 인프라에 대한 이해**입니다. 지난 포스트에서 설명했듯이 잘못된 지점을 모니터링하거나 중요한 지점을 관리하지 못한다면 예상치 못한 장애로 failover가 발생할 수 있습니다.

따라서 NAS 이중화는 스토리지에 대한 기능과 파일 공유를 위한 다양한 요소들을 빠짐없이 모니터링해야하기 때문에 다른 이중화 서비스에 비교해서 보다 높은 기술 지식과 환경에 대한 이해를 요구합니다.

&nbsp;

## NAS 이중화 사례 설명

그렇다면 실제 구축된 사례를 예시로 NAS 이중화 아키텍쳐를 설계하는 과정을 설명해보겠습니다.

&nbsp;

![Alt text](/assets/HA2_FIG1.jpg){: width="500"}
<center>&#60;VDI 환경을 위한 NAS 이중화 시스템 구성도&#62;</center>

&nbsp;

위 그림은 ㅎ사에 구축된 업무 환경을 위한 VDI(Virtual Desktop Infrastructure) 기반의 시스템 구성도이며, 고객사에서 요구하는 기능은 다음과 같습니다.

> * VDI 클러스터는 가상머신의 이미지를 저장할 용도로 NFS 공유 환경을 제공받아야 함
> * 서비스 도중에 failover가 발생하더라도 실행중인 가상머신에 영향이 없어야 함
> * 가상머신에 설치된 운영체제는 윈도우즈 기반이며, CIFS(Common Interest File System) 공유 환경을 제공받아야 함
> * 계정 정보는 AD(Active Directory)로 통합 관리되며, NAS는 AD와 연동하여 사용자 인증을 처리해야 함
> * 동시 접속자는 약 1000명이며, 1000대의 가상머신과 파일 공유 환경을 동시에 제공해야 함

ㅎ사의 환경은 조금 특별했습니다. 고객사는 기존에 가상화 기술과 스토리지가 연동된 HCI(Hyper-Converged Infrastructure) 장비를 통해 가상머신의 이미지를 클러스터 스토리지에서 관리하고, 가상머신으로 NAS를 구축하여 사용자 파일을 공유하고 있었습니다. 하지만 잦은 장애와 부족한 NAS 성능으로 원활한 업무 환경 제공에 많은 노력이 필요했다고 합니다.

이번 VDI 구축 사업에는 인프라 구성에 필요한 가상머신 이미지와 개인 사용자 파일을 별도의 고성능 스토리지에 저장하고 공유하기를 원했습니다. 이 경우 NAS에 문제가 발생한다면 사용자 파일 공유뿐만 아니라, 인프라 전체가 다운될 수 있어서 NAS 이중화 환경 구성은 필수였습니다. 만약 장애가 발생하여 Standby 스토리지에 서비스가 이전(failover)되더라도 실행중인 가상머신에는 영향이 없어야 하기 때문에 런타임 서비스 이전(runtime failover)이 가능한 고성능 NAS 이중화 기술이 필요했습니다.

&nbsp;

![Alt text](/assets/HA2_FIG4.jpg){: width="500"}
<center>&#60;(좌)VDI 환경의 입출력 패턴 / (우)평균 IOPS 기준에 따른 성능 체감 기준&#62;</center>

&nbsp;

가상화 환경은 real-time small data 패턴의 입출력 요청(I/O Request)이 발생합니다. ㅎ사는 별도의 사용자 데이터가 저장될 공간을 가상머신 이미지로부터 분리했기 때문에 가상머신의 대부분의 입출력 요청은 사용자 파일을 위한 요청이 아닌 게스트 운영체제에서 요청한 것이며, 그 단위는 작게는 bytes에서 KB 정도의 크기입니다. 이번 구축 사례에서는 ㅎ사의 입출력 패턴에 최적화 하기위해 기존의 파일시스템 기반의 클러스터링 기술이 아닌 블록 기반의 데이터 미러링 기술을 사용했습니다. 클러스터 파일시스템의 공유 단위가 파일이라면, 블록 기반의 데이터 미러링은 실제 디스크에 저장되는 블록 단위로 저장되는 차이점이 있습니다.

&nbsp;

![Alt text](/assets/HA2_FIG2.jpg){: width="500"}
<center>&#60;블록 기반의 데이터 미러링 기술의 입출력 흐름도&#62;</center>

&nbsp;

위 그림은 블록 기반의 데이터 미러링 기술의 입출력 흐름도 입니다. 스토리지는 Active 노드에서 NAS 서비스를 수행하고 있으며, 클라이언트의 파일 공유 요청은 SMB와 NFS와 같은 공유 프로토콜과 다양한 레이어를 지나 소프트웨어 계층의 최하단인 데이터 미러링 스택에 도착합니다(1). 데이터 미러링에 전달된 블록 입출력 요청은 Active 노드의 디스크에서 처리하는 동시에 미러링된 Standby 노드에도 전달합니다(2). Standby 노드의 데이터 미러링 스택은 로컬 디스크에도 동일한 블록 입출력 요청을 처리하고 그 결과를 Active 노드에 전달합니다(4). Active 노드의 데이터 미러링 스택은 입출력 결과를 상위 계층에 전달하며(5) 입출력 요청을 완료합니다.

데이터 미러링 기술에서 가장 중요한 이슈는 성능과 안정성 입니다. 먼저 이중화된 노드 사이에 데이터 미러링에 소요되는 지연시간(Latency)을 최소화 하기위해 스토리지 미러링 네트워크는 인피니밴드(Infiniband)를 사용했습니다. 그리고 약1000명의 VM과 사용자로부터 발생하는 Random Access 패턴에 대한 소요되는 시간을 최소화하기위해 검색 시간(Seek Time)이 낮은 SSD로 디스크를 구성하여 저지연성을 확보했습니다. 이때 시스탬 내부의 장애 상황을 대비하여 인피니밴드는 본딩(Bonding) 기술을 통해 이중화로 구성했으며, SSD는 RAID 10로 구성하여 IOPS(Input/Ouput Operations Per Second)를 높이고 패리티 연산으로 소모되는 플래시 메모리 수명을 최소화하여 성능과 안정성을 확보했습니다.

&nbsp;

![Alt text](/assets/HA2_FIG3.jpg){: width="500"}
<center>&#60;ㅎ사에 구축된 NAS 이중화 아키텍쳐&#62;</center>

&nbsp;

다음으로 고성능 NAS 이중화 구성에 필요한 모니터링 요소와 관계에 대해서 설명하겠습니다. 위 구조도는 ㅎ사에 구축된 NAS 이중화 시스템의 모니터링 아키텍처 입니다. 모니터링 요소간의 배치는 입출력 요청에 대한 처리 순서와 요소들간의 관계에 따라서 결정되며, 각 모니터링 요소에 대한 설명은 아래와 같습니다.

> 1. Virtual IP - 클라이언트가 접속하고자하는 서비스 IP 할당
> 2. Ethernet Interface - Virtual IP가 할당된 네트워크 인터페이스의 상태 확인
> 3. AD Connector - SMB와 NFS에서 사용할 계증 인증 커넥터
> 4. NFS와SMB 서비스의 상태 확인
> 5. Filesystem - 파일시스템 R/W 가능 여부 확인
> 6. LVM - 볼륨 그룹 및 그룹에 속하는 블록 장치의 상태 확인
> 7. DRBD - DRBD 클러스터 상태 확인

&nbsp;

![Alt text](/assets/HA2_FIG5.jpg){: width="500"}
<center>&#60;잘못된 아키텍쳐 설계 예시&#62;</center>

&nbsp;

이때 주의해야 할 부분은 모니터링 요소의 배치에 따라 서비스의 연속성 보장에 실패할 수 있습니다. 한 예로 클라이언트가 접속할 네트워크 주소(IP)를 할당하는 Virtual IP 에이전트가 파일 공유 프로토콜 스택인 NFS와 SMB보다 아래에 있다면 failover 발생 시 클라이언트 접속이 끊기게 됩니다. 클라이언트 입장에서 위 그림의 두 구조에 따른 차이는 다음과 같습니다.

> - A 구조
>   > IP가 할당 된 시점에 이미 파일 공유 데몬이 준비되었기 때문에 통신이 재개된 후에 입출력 요청 큐에 대기중인 작업들도 모두 데몬과 통신하여 처리됨
> - B 구조
>   > IP가 할당 된 시점에 파일 공유 데몬이 준비되지 않아 통신이 재개된 클라이언트들의 입출력 요청을 처리하지 못함

이는 에이전트 실행 순서가 Bottom-Up 방식이기 때문입니다. 즉, 파일 공유 데몬이 실행되지 않았는데 IP가 먼저 할당되면 서버와 연결이 끊겼던 클라이언트가 재연결된 IP로 통신을 시도하지만 관련 프로토콜을 처리하는 데몬들이 시작하기 전이기 때문에 입출력 요청 큐(I/O Request Queue)에 대기중인 작업들이 모두 실패하게 됩니다.

파일 공유 데몬이 준비가 된 후에 IP가 할당되어 클라이언트가 연결되면서 통신이 재개됨


위 구조도와 같은 순서라면 SMB와 NFS 데몬이 먼저 Standby 노드에 이전된 후에 IP가 할당되기 때문에 failover되면서 잠시 홀딩되었던 클라이언트 프로토콜들이 failover된 Standby 노드의 IP로 찾아가면서 SMB와 NFS를 재연결하게 됩니다.

구조도의 모니터링 스택에서 서비스 처리 과정은 가장 상위 스택부터 내려가면서 처리되지만, 반대로 failover 과정이나 서비스 재시작과 같은 상황에서는 서비스 구동 순서는 가장 하위 스택에서 올라가면서 시작되게 됩니다. 만약 클라이언트가 접속할 네트워크 주소(IP)를 할당하는 Virtual IP 에이전트가 파일 공유 프로토콜 스택인 NFS와 SMB이전에 처리되게 된다면 문제가 생길 수 있습니다.


그러면 구조도를 기준으로 클라이언트로부터 파일 공유 요청이 입력될 때 처리되는 과정을 살펴보겠습니다.
서비스 처리 순서는 Top-Down 방식으로 상위 스택부터 아래로 차례대로 처리됩니다. 클라이언트는 접속할 스토리지의 주소를 찾게되며, 이는 Virtual IP 에이전트에서 모니터링합니다. Virtual IP 에이전트는 입력된 네트워크 인터페이스에 IP를 할당하는 역할을 수행하며, 해당 인터페이스를 주기적으로 모니터링하여 IP 할당 여부를 확인합니다. 접속한 Virtual IP는 클라이언트가 접속하는 IP를 관리하는 요청으로

반대로 서비스가 시작되는 순서는 Bottom-Up 방식으로 하위 스택부터 위로 차례대로 시작됩니다. 서비스 시작은 시스템 재시작이나 failover 과정에서 각 기능이 서비스가 가능하도록  시작하는 순서를 뜻합니다.

상단에서 하단까지 Top-Down 방식으로 입출력 요청이 처리됩니다. 이는 반대로 뜻하면 각 스택은 이어서 모니터링 요소의 상위부터 차례대로 스택의 기능과 관계를 설명해보겠습니다


위 그림의 최하단은 올 플래시로 구성된 RAID 볼륨 장치이며, 데이터 미러링 스택은 블록 장치에 가장 먼저 연결됩니다. 데이터 미러링 기술에는 리눅스에서 블록 기반의 데이터 이중화 기술로 인정받은 DRBD(Distributed Replicasted Storage System)을 사용합니다(DRBD에 대한 설명은 다른 포스트에서 자세히 다루도록 하겠습니다.) DRBD는 앞서 설명한 바와 같이 인피니밴드를 이용하여 지연시간을 최소화 하였으며, 확보된 성능을 활용하여 모든 입출력 요청이 완료된 이후에 어플리케이션에 완료 응답을 송신하는 안정적인 구조인 DRBD Protocol C 타입으로 미러링했습니다.

다음으로 ㅎ사에 도입된 NAS 이중화 시스템의 모니터링 지점과 소프트웨어 스택에 대한 순서와 관계성에 대해서 설명하겠습니다. 먼저 도입된 소프트웨어의 순서는 다음과 같습니다.

~~실제로 클러스터 파일시스템을 통해 데모 환경을 구성했을 때, failover 과정에서 가상머신에 블루스크린이 발생하여 시스템이 다운되었습니다.~~
하지만 블록 기반의 미러


위 환경은 조금 특별한 상황입니다. 먼저 

(사례를 통한 아키텍처 설명)
1. KT 사례 (모바일 스트리밍, 167TB, SAN, NFS, 40G, ethmonitor, FCmonitor, A/B)

2. 센트럴넷 사례 (대용량 미디어, 잔파일, SAN, Gluster, A/A)

3. 지역난방공사 사례 (VDI, All Flash Storage, AD, SMB, NFS, DRBD, A/B)

4. 심평원 사례 (빅데이터, 잔파일, SAN, NFS, personal_cmd,  A/B)

![Alt text](/assets/???.jpg){: width="500"}
<center>&#60;모바일 스트리밍 환경을 위한 A/B NAS 이중화 구축&#62;</center>

&nbsp;

## 이중화를 위한 오픈소스 프로젝트

(Pacemaker 소개)
(Pacemaker 기능 소개)
(Pacemaker를 통한 스토리지 이중화 환경 구축 실습-간단하게)

&nbsp;

## 마치며

(마무리 멘트)
이번 포스트에서는 사례별 NAS 이중화 아키텍처에 대해서 설명했으며, 이중화를 위한 오픈소스 프로젝트인 Pacemaker에 대한 소개와 핵심 기능들을 간단하게 소개했습니다. 그리고 앞서 설계한 아키텍처를 목표로 환경을 구축을 실습해보았습니다.

다음 포스트는 Pacemaker 심화 과정으로 실제 현장에서 발생한 이슈들과 이를 해결하기 위한 기술적인 접근 방법들을 공유하겠습니다. 더 나아가 특수 환경에서 중요한 지점을 모니터링하기 위해 Pacemaker 리소스를 개발하는 방법과 현재까지 내부에서 개발한 몇가지 리소스를 공개하도록 하겠습니다. 감사합니다.

&nbsp;

## 참고

 * https://www.netapp.com/media/19785-wp-sizing-storage-for-desktop.pdf
 * https://www.infortrend.com/ImageLoader/LoadDoc/509/True/True/Infortrend%20document

## 각주

[^1]: https://???
